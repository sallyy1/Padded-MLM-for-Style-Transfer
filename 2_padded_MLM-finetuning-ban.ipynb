{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a8ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyunkyung_lee/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "##from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b28fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, AdamW\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")#, use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c1fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "###model = AutoModelForMaskedLM.from_pretrained('spanbert-base-cased')\n",
    "\n",
    "\n",
    "# model_name = \"SpanBERT/spanbert-base-cased\" \n",
    "\n",
    "\n",
    "# # Download pytorch model\n",
    "# model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0b241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "corpus_mun = []\n",
    "corpus_ban = []\n",
    "\n",
    "with open('train_sample.tsv', 'r') as fp:\n",
    "    text = fp.read().split('\\n')\n",
    "    \n",
    "for line in text:\n",
    "    sentence = line.split('\\t')[0]\n",
    "    corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cc153c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016년에 갤럭시 S7 에지가 폭발한 사건은 어느 지역에서 일어났는가?\\t0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[196326]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745993a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'중국에서 아파트에서 추락하던 3세 아이를 살리고 자신은 혼수상태에 빠진 사람은 누구야?\\t1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[196327]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ebf60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'천중핑씨가 추락하는 아이를 구하고 뇌출혈로 인한 의식불명 상태에 빠진 건 언제야?\\t1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[196328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1da030",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_mun = corpus[:196327]\n",
    "corpus_ban = corpus[196327:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14fde3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196327\n",
      "212423\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_mun))\n",
    "print(len(corpus_ban))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c567d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번 코퍼스 학습 (1. 문어체 수행)\n",
    "\n",
    "###inputs = tokenizer(corpus_mun, return_tensors='pt', max_length=128, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5f74b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2번 코퍼스 학습 (2. 반말체 수행)\n",
    "\n",
    "inputs = tokenizer(corpus_ban, return_tensors='pt', max_length=128, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d6c353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  3693, 27135,  ...,     0,     0,     0],\n",
       "        [    2,  1652,  2284,  ...,     0,     0,     0],\n",
       "        [    2,  1652,  2284,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,  4631,  2145,  ...,     0,     0,     0],\n",
       "        [    2,  3907,  2073,  ...,     0,     0,     0],\n",
       "        [    2,     3,     0,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e092c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ff916b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  3693, 27135,  ...,     0,     0,     0],\n",
       "        [    2,  1652,  2284,  ...,     0,     0,     0],\n",
       "        [    2,  1652,  2284,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,  4631,  2145,  ...,     0,     0,     0],\n",
       "        [    2,  3907,  2073,  ...,     0,     0,     0],\n",
       "        [    2,     3,     0,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 0]]), 'labels': tensor([[    2,  3693, 27135,  ...,     0,     0,     0],\n",
       "        [    2,  1652,  2284,  ...,     0,     0,     0],\n",
       "        [    2,  1652,  2284,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,  4631,  2145,  ...,     0,     0,     0],\n",
       "        [    2,  3907,  2073,  ...,     0,     0,     0],\n",
       "        [    2,     3,     0,  ...,     0,     0,     0]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bf82c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([212423, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "rand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2e7edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7713, 0.7832, 0.4905,  ..., 0.1915, 0.0277, 0.1315],\n",
       "        [0.3432, 0.7173, 0.6572,  ..., 0.2095, 0.6183, 0.8945],\n",
       "        [0.0226, 0.8103, 0.2771,  ..., 0.0113, 0.0226, 0.2871],\n",
       "        ...,\n",
       "        [0.6689, 0.7133, 0.4423,  ..., 0.4455, 0.5255, 0.1201],\n",
       "        [0.0678, 0.5768, 0.7129,  ..., 0.9574, 0.5518, 0.3828],\n",
       "        [0.7945, 0.3680, 0.4701,  ..., 0.8705, 0.7858, 0.4045]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fee0386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr = rand < 0 #0.15\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3718a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs.input_ids != 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b477b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810d625",
   "metadata": {},
   "source": [
    "# - MASKING 방법에 변화 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0c7f099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"서울과 충북 괴산에서 '국제 청소년포럼'을 여는 곳은?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_mun[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a1d98c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3671,\n",
       " 2145,\n",
       " 7249,\n",
       " 25859,\n",
       " 27135,\n",
       " 11,\n",
       " 3854,\n",
       " 4857,\n",
       " 2208,\n",
       " 2731,\n",
       " 11,\n",
       " 1498,\n",
       " 1428,\n",
       " 2259,\n",
       " 601,\n",
       " 2073,\n",
       " 35,\n",
       " 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(corpus_mun[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87006418",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2f5f99d7edc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encoded' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "415a4214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] 서울과 충북 괴산에서'국제 청소년포럼'을 여는 곳은? [SEP]\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "910c5fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3671,\n",
       " 2145,\n",
       " 7249,\n",
       " 25859,\n",
       " 27135,\n",
       " 11,\n",
       " 3854,\n",
       " 4857,\n",
       " 2208,\n",
       " 2731,\n",
       " 11,\n",
       " 1498,\n",
       " 1428,\n",
       " 2259,\n",
       " 601,\n",
       " 2073,\n",
       " 35,\n",
       " 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.convert_ids_to_tokens(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ce889bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 토큰화 결과 : ['[CLS]', '서울', '##과', '충북', '괴산', '##에서', \"'\", '국제', '청소년', '##포', '##럼', \"'\", '을', '여', '##는', '곳', '##은', '?', '[SEP]']\n",
      "\n",
      "*** <문장으로 보기> : [CLS] 서울과 충북 괴산에서'국제 청소년포럼'을 여는 곳은? [SEP]\n",
      "\n",
      "*** 정수 인코딩 : [2, 3671, 2145, 7249, 25859, 27135, 11, 3854, 4857, 2208, 2731, 11, 1498, 1428, 2259, 601, 2073, 35, 3]\n",
      "\n",
      "*** 디코딩 : [CLS] 서울과 충북 괴산에서'국제 청소년포럼'을 여는 곳은? [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(corpus_mun[0]) # str() 타입의 문장을 인풋\n",
    "print('*** 토큰화 결과 :',tokenizer.convert_ids_to_tokens(encoded), end='\\n\\n')\n",
    "print('*** <문장으로 보기> :', tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded)), end='\\n\\n')\n",
    "print('*** 정수 인코딩 :',encoded, end='\\n\\n') # encoded.ids\n",
    "print('*** 디코딩 :',tokenizer.decode(encoded), end='\\n\\n') # encoded.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a71107d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  3693, 27135,  4155, 27135,  8494,  2205,  2414,    23,  2103,\n",
       "         3651,  2138,  6063,  2088,  3638,  2073, 19666,  2290,  2260,  2170,\n",
       "         6830,  3611,  2073,  4061,  2275,    35,     3,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "986c89eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.encode(corpus_mun[0]))) # 총 19개 토큰\n",
    "print(len(inputs.input_ids[0])) # 전체 길이는 max_len = 128, 그중 패딩이 아닌 토큰은 총 19개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6107112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 토큰화 결과 : ['[CLS]', '티', '##베', '##로', '##의', '가장', '큰', '장점', '##은', '무엇', '##인', '##가', '?', '[SEP]']\n",
      "\n",
      "*** <문장으로 보기> : [CLS] 티베로의 가장 큰 장점은 무엇인가? [SEP]\n",
      "\n",
      "*** 정수 인코딩 : [2, 1819, 2472, 2200, 2079, 3676, 1751, 5472, 2073, 3890, 2179, 2116, 35, 3]\n",
      "\n",
      "*** 디코딩 : [CLS] 티베로의 가장 큰 장점은 무엇인가? [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('티베로의 가장 큰 장점은 무엇인가?') # str() 타입의 문장을 인풋\n",
    "print('*** 토큰화 결과 :',tokenizer.convert_ids_to_tokens(encoded), end='\\n\\n')\n",
    "print('*** <문장으로 보기> :', tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded)), end='\\n\\n')\n",
    "print('*** 정수 인코딩 :',encoded, end='\\n\\n') # encoded.ids\n",
    "print('*** 디코딩 :',tokenizer.decode(encoded), end='\\n\\n') # encoded.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "196a474c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.encode('티베로의 가장 큰 장점은 무엇인가?'))) # 총 14개 토큰\n",
    "###print(len(inputs.input_ids[0])) # 전체 길이는 max_len = 128, 그중 패딩이 아닌 토큰은 총 14개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c83dc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 토큰화 결과 : ['[CLS]', '티', '##베', '##로', '##의', '가장', '큰', '장점', '##은', '뭐', '##야', '?', '[SEP]']\n",
      "\n",
      "*** <문장으로 보기> : [CLS] 티베로의 가장 큰 장점은 뭐야? [SEP]\n",
      "\n",
      "*** 정수 인코딩 : [2, 1819, 2472, 2200, 2079, 3676, 1751, 5472, 2073, 1097, 2275, 35, 3]\n",
      "\n",
      "*** 디코딩 : [CLS] 티베로의 가장 큰 장점은 뭐야? [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('티베로의 가장 큰 장점은 뭐야?') # str() 타입의 문장을 인풋\n",
    "print('*** 토큰화 결과 :',tokenizer.convert_ids_to_tokens(encoded), end='\\n\\n')\n",
    "print('*** <문장으로 보기> :', tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded)), end='\\n\\n')\n",
    "print('*** 정수 인코딩 :',encoded, end='\\n\\n') # encoded.ids\n",
    "print('*** 디코딩 :',tokenizer.decode(encoded), end='\\n\\n') # encoded.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aafe95",
   "metadata": {},
   "source": [
    "## - 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c25a83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 토큰화 결과 : ['[CLS]', '[MASK]', '[SEP]']\n",
      "\n",
      "*** <문장으로 보기> : [CLS] [MASK] [SEP]\n",
      "\n",
      "*** 정수 인코딩 : [2, 4, 3]\n",
      "\n",
      "*** 디코딩 : [CLS] [MASK] [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('[MASK]') # str() 타입의 문장을 인풋\n",
    "print('*** 토큰화 결과 :',tokenizer.convert_ids_to_tokens(encoded), end='\\n\\n')\n",
    "print('*** <문장으로 보기> :', tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded)), end='\\n\\n')\n",
    "print('*** 정수 인코딩 :',encoded, end='\\n\\n') # encoded.ids\n",
    "print('*** 디코딩 :',tokenizer.decode(encoded), end='\\n\\n') # encoded.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba469476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 토큰화 결과 : ['[CLS]', '서울', '##과', '충북', '괴산', '##에서', \"'\", '국제', '청소년', '##포', '##럼', \"'\", '을', '여', '##는', '[MASK]', '[MASK]', '?', '[SEP]']\n",
      "\n",
      "*** <문장으로 보기> : [CLS] 서울과 충북 괴산에서'국제 청소년포럼'을 여는 [MASK] [MASK]? [SEP]\n",
      "\n",
      "*** 정수 인코딩 : [2, 3671, 2145, 7249, 25859, 27135, 11, 3854, 4857, 2208, 2731, 11, 1498, 1428, 2259, 4, 4, 35, 3]\n",
      "\n",
      "*** 디코딩 : [CLS] 서울과 충북 괴산에서'국제 청소년포럼'을 여는 [MASK] [MASK]? [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('서울과 충북 괴산에서 \\'국제 청소년포럼\\'을 여는 곳은?') # str() 타입의 문장을 인풋\n",
    "encoded[-3] = 4 # '[MASK]' 토큰\n",
    "encoded[-4] = 4 # '[MASK]' 토큰\n",
    "\n",
    "print('*** 토큰화 결과 :',tokenizer.convert_ids_to_tokens(encoded), end='\\n\\n')\n",
    "print('*** <문장으로 보기> :', tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded)), end='\\n\\n')\n",
    "print('*** 정수 인코딩 :',encoded, end='\\n\\n') # encoded.ids\n",
    "print('*** 디코딩 :',tokenizer.decode(encoded), end='\\n\\n') # encoded.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a1733a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  ..., False, False, False],\n",
       "        [False,  True,  True,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_arr = (rand < 0.15) * (inputs.input_ids != 2) * (inputs.input_ids != 3)\n",
    "# mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3492a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "973c06d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [17],\n",
       "        [18],\n",
       "        [19],\n",
       "        [20],\n",
       "        [21],\n",
       "        [22],\n",
       "        [23],\n",
       "        [24],\n",
       "        [25],\n",
       "        [26]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da66c003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(inputs.input_ids[0].nonzero()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9abac23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"서울과 충북 괴산에서 '국제 청소년포럼'을 여는 곳은?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_mun[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d08c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_mun[0]))\n",
    "print(len(torch.flatten(mask_arr[0].nonzero()).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd8ad2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(mask_arr.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa6878f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], [], [], [], [], [], []]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "700b2d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  3671,  2145,  7249, 25859, 27135,    11,  3854,  4857,  2208,\n",
       "         2731,    11,  1498,  1428,  2259,   601,  2073,    35,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e74c7ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] 서울과 충북 괴산에서'국제 청소년포럼'을 여는 곳은? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "674d4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(mask_arr.shape[0]):\n",
    "#     inputs.input_ids[i, selection[i]] = 3 # [SEP] 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a34261fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  3671,  2145,  7249, 25859, 27135,    11,  3854,  4857,  2208,\n",
       "         2731,    11,  1498,  1428,  2259,   601,  2073,    35,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5781eb4e",
   "metadata": {},
   "source": [
    "# - 어텐션 마스크 변형주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec89bfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d778ce67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.token_type_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e13dc02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs.attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66132d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.attention_mask.nonzero()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9fcd7",
   "metadata": {},
   "source": [
    "- 1. 문어체 학습(fine-tuning)용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdb9e550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 14,\n",
       " 8,\n",
       " 10,\n",
       " 18,\n",
       " 22,\n",
       " 18,\n",
       " 19,\n",
       " 16,\n",
       " 18,\n",
       " 30,\n",
       " 25,\n",
       " 13,\n",
       " 16,\n",
       " 13,\n",
       " 15,\n",
       " 11,\n",
       " 18,\n",
       " 25,\n",
       " 18,\n",
       " 18,\n",
       " 16,\n",
       " 20,\n",
       " 16,\n",
       " 22,\n",
       " 27,\n",
       " 21,\n",
       " 18,\n",
       " 9,\n",
       " 43,\n",
       " 32,\n",
       " 18,\n",
       " 57,\n",
       " 11,\n",
       " 24,\n",
       " 28,\n",
       " 15,\n",
       " 14,\n",
       " 27,\n",
       " 13,\n",
       " 17,\n",
       " 15,\n",
       " 19,\n",
       " 13,\n",
       " 19,\n",
       " 17,\n",
       " 21,\n",
       " 31,\n",
       " 24,\n",
       " 23,\n",
       " 21,\n",
       " 46,\n",
       " 27,\n",
       " 13,\n",
       " 15,\n",
       " 16,\n",
       " 38,\n",
       " 20,\n",
       " 21,\n",
       " 15,\n",
       " 19,\n",
       " 16,\n",
       " 12,\n",
       " 13,\n",
       " 17,\n",
       " 14,\n",
       " 17,\n",
       " 21,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 37,\n",
       " 15,\n",
       " 12,\n",
       " 30,\n",
       " 26,\n",
       " 29,\n",
       " 32,\n",
       " 28,\n",
       " 19,\n",
       " 26,\n",
       " 18,\n",
       " 25,\n",
       " 30,\n",
       " 29,\n",
       " 19,\n",
       " 17,\n",
       " 17,\n",
       " 22,\n",
       " 15,\n",
       " 16,\n",
       " 12,\n",
       " 18,\n",
       " 19,\n",
       " 16,\n",
       " 20,\n",
       " 20,\n",
       " 14,\n",
       " 25,\n",
       " 21,\n",
       " 15,\n",
       " 26,\n",
       " 18,\n",
       " 26,\n",
       " 18,\n",
       " 22,\n",
       " 63,\n",
       " 24,\n",
       " 35,\n",
       " 40,\n",
       " 15,\n",
       " 22,\n",
       " 47,\n",
       " 46,\n",
       " 23,\n",
       " 20,\n",
       " 23,\n",
       " 31,\n",
       " 27,\n",
       " 29,\n",
       " 18,\n",
       " 12,\n",
       " 20,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 17,\n",
       " 21,\n",
       " 16,\n",
       " 12,\n",
       " 19,\n",
       " 23,\n",
       " 24,\n",
       " 16,\n",
       " 11,\n",
       " 17,\n",
       " 23,\n",
       " 26,\n",
       " 31,\n",
       " 17,\n",
       " 24,\n",
       " 35,\n",
       " 27,\n",
       " 17,\n",
       " 25,\n",
       " 16,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 13,\n",
       " 21,\n",
       " 17,\n",
       " 21,\n",
       " 18,\n",
       " 13,\n",
       " 11,\n",
       " 28,\n",
       " 22,\n",
       " 38,\n",
       " 17,\n",
       " 40,\n",
       " 15,\n",
       " 19,\n",
       " 12,\n",
       " 16,\n",
       " 13,\n",
       " 18,\n",
       " 19,\n",
       " 26,\n",
       " 21,\n",
       " 35,\n",
       " 24,\n",
       " 21,\n",
       " 27,\n",
       " 33,\n",
       " 25,\n",
       " 26,\n",
       " 17,\n",
       " 19,\n",
       " 31,\n",
       " 10,\n",
       " 17,\n",
       " 11,\n",
       " 21,\n",
       " 9,\n",
       " 11,\n",
       " 14,\n",
       " 15,\n",
       " 20,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 11,\n",
       " 16,\n",
       " 21,\n",
       " 11,\n",
       " 22,\n",
       " 9,\n",
       " 14,\n",
       " 35,\n",
       " 19,\n",
       " 11,\n",
       " 17,\n",
       " 10,\n",
       " 26,\n",
       " 10,\n",
       " 29,\n",
       " 37,\n",
       " 20,\n",
       " 11,\n",
       " 24,\n",
       " 21,\n",
       " 12,\n",
       " 19,\n",
       " 15,\n",
       " 50,\n",
       " 14,\n",
       " 19,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 29,\n",
       " 25,\n",
       " 20,\n",
       " 17,\n",
       " 21,\n",
       " 27,\n",
       " 21,\n",
       " 22,\n",
       " 16,\n",
       " 21,\n",
       " 11,\n",
       " 18,\n",
       " 13,\n",
       " 16,\n",
       " 16,\n",
       " 18,\n",
       " 18,\n",
       " 9,\n",
       " 17,\n",
       " 21,\n",
       " 23,\n",
       " 16,\n",
       " 18,\n",
       " 16,\n",
       " 20,\n",
       " 28,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 20,\n",
       " 22,\n",
       " 10,\n",
       " 29,\n",
       " 25,\n",
       " 32,\n",
       " 22,\n",
       " 11,\n",
       " 32,\n",
       " 11,\n",
       " 59,\n",
       " 23,\n",
       " 27,\n",
       " 13,\n",
       " 20,\n",
       " 24,\n",
       " 35,\n",
       " 15,\n",
       " 18,\n",
       " 21,\n",
       " 12,\n",
       " 33,\n",
       " 22,\n",
       " 20,\n",
       " 27,\n",
       " 41,\n",
       " 18,\n",
       " 19,\n",
       " 27,\n",
       " 50,\n",
       " 18,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 27,\n",
       " 24,\n",
       " 24,\n",
       " 22,\n",
       " 12,\n",
       " 16,\n",
       " 25,\n",
       " 27,\n",
       " 62,\n",
       " 70,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 26,\n",
       " 13,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 16,\n",
       " 17,\n",
       " 29,\n",
       " 18,\n",
       " 24,\n",
       " 17,\n",
       " 20,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 34,\n",
       " 16,\n",
       " 10,\n",
       " 20,\n",
       " 17,\n",
       " 13,\n",
       " 13,\n",
       " 23,\n",
       " 19,\n",
       " 16,\n",
       " 18,\n",
       " 16,\n",
       " 20,\n",
       " 10,\n",
       " 17,\n",
       " 20,\n",
       " 17,\n",
       " 25,\n",
       " 14,\n",
       " 26,\n",
       " 19,\n",
       " 23,\n",
       " 20,\n",
       " 25,\n",
       " 30,\n",
       " 11,\n",
       " 28,\n",
       " 25,\n",
       " 28,\n",
       " 24,\n",
       " 24,\n",
       " 16,\n",
       " 16,\n",
       " 22,\n",
       " 11,\n",
       " 15,\n",
       " 25,\n",
       " 17,\n",
       " 14,\n",
       " 18,\n",
       " 21,\n",
       " 41,\n",
       " 64,\n",
       " 19,\n",
       " 11,\n",
       " 25,\n",
       " 15,\n",
       " 22,\n",
       " 19,\n",
       " 37,\n",
       " 32,\n",
       " 24,\n",
       " 19,\n",
       " 14,\n",
       " 17,\n",
       " 17,\n",
       " 22,\n",
       " 16,\n",
       " 9,\n",
       " 15,\n",
       " 19,\n",
       " 17,\n",
       " 20,\n",
       " 18,\n",
       " 15,\n",
       " 13,\n",
       " 22,\n",
       " 27,\n",
       " 14,\n",
       " 14,\n",
       " 30,\n",
       " 19,\n",
       " 20,\n",
       " 17,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 16,\n",
       " 20,\n",
       " 17,\n",
       " 18,\n",
       " 26,\n",
       " 19,\n",
       " 23,\n",
       " 23,\n",
       " 16,\n",
       " 20,\n",
       " 9,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 10,\n",
       " 13,\n",
       " 19,\n",
       " 20,\n",
       " 16,\n",
       " 35,\n",
       " 12,\n",
       " 23,\n",
       " 22,\n",
       " 37,\n",
       " 29,\n",
       " 38,\n",
       " 34,\n",
       " 19,\n",
       " 74,\n",
       " 57,\n",
       " 17,\n",
       " 23,\n",
       " 20,\n",
       " 16,\n",
       " 24,\n",
       " 27,\n",
       " 24,\n",
       " 26,\n",
       " 17,\n",
       " 10,\n",
       " 24,\n",
       " 11,\n",
       " 20,\n",
       " 23,\n",
       " 16,\n",
       " 10,\n",
       " 14,\n",
       " 17,\n",
       " 13,\n",
       " 19,\n",
       " 12,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 21,\n",
       " 21,\n",
       " 31,\n",
       " 30,\n",
       " 25,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 19,\n",
       " 9,\n",
       " 45,\n",
       " 18,\n",
       " 25,\n",
       " 23,\n",
       " 22,\n",
       " 17,\n",
       " 18,\n",
       " 12,\n",
       " 19,\n",
       " 18,\n",
       " 23,\n",
       " 22,\n",
       " 25,\n",
       " 12,\n",
       " 33,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 9,\n",
       " 15,\n",
       " 10,\n",
       " 16,\n",
       " 10,\n",
       " 21,\n",
       " 18,\n",
       " 19,\n",
       " 21,\n",
       " 15,\n",
       " 38,\n",
       " 23,\n",
       " 33,\n",
       " 19,\n",
       " 26,\n",
       " 20,\n",
       " 9,\n",
       " 21,\n",
       " 38,\n",
       " 43,\n",
       " 17,\n",
       " 34,\n",
       " 26,\n",
       " 18,\n",
       " 19,\n",
       " 12,\n",
       " 20,\n",
       " 22,\n",
       " 10,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 16,\n",
       " 19,\n",
       " 28,\n",
       " 32,\n",
       " 22,\n",
       " 17,\n",
       " 18,\n",
       " 24,\n",
       " 17,\n",
       " 15,\n",
       " 39,\n",
       " 14,\n",
       " 13,\n",
       " 20,\n",
       " 11,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 20,\n",
       " 18,\n",
       " 12,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 17,\n",
       " 24,\n",
       " 33,\n",
       " 27,\n",
       " 28,\n",
       " 11,\n",
       " 23,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 10,\n",
       " 14,\n",
       " 13,\n",
       " 24,\n",
       " 18,\n",
       " 12,\n",
       " 18,\n",
       " 21,\n",
       " 24,\n",
       " 20,\n",
       " 22,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 26,\n",
       " 21,\n",
       " 18,\n",
       " 19,\n",
       " 21,\n",
       " 34,\n",
       " 46,\n",
       " 30,\n",
       " 33,\n",
       " 22,\n",
       " 19,\n",
       " 23,\n",
       " 21,\n",
       " 19,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 14,\n",
       " 20,\n",
       " 27,\n",
       " 34,\n",
       " 20,\n",
       " 19,\n",
       " 16,\n",
       " 29,\n",
       " 19,\n",
       " 13,\n",
       " 32,\n",
       " 32,\n",
       " 17,\n",
       " 23,\n",
       " 26,\n",
       " 22,\n",
       " 20,\n",
       " 16,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 18,\n",
       " 33,\n",
       " 28,\n",
       " 24,\n",
       " 35,\n",
       " 19,\n",
       " 38,\n",
       " 34,\n",
       " 11,\n",
       " 26,\n",
       " 16,\n",
       " 17,\n",
       " 21,\n",
       " 31,\n",
       " 11,\n",
       " 26,\n",
       " 21,\n",
       " 15,\n",
       " 17,\n",
       " 15,\n",
       " 15,\n",
       " 27,\n",
       " 20,\n",
       " 14,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 16,\n",
       " 16,\n",
       " 25,\n",
       " 26,\n",
       " 18,\n",
       " 20,\n",
       " 15,\n",
       " 15,\n",
       " 17,\n",
       " 15,\n",
       " 16,\n",
       " 26,\n",
       " 11,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 17,\n",
       " 30,\n",
       " 22,\n",
       " 10,\n",
       " 11,\n",
       " 13,\n",
       " 23,\n",
       " 10,\n",
       " 10,\n",
       " 45,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 17,\n",
       " 18,\n",
       " 30,\n",
       " 20,\n",
       " 22,\n",
       " 28,\n",
       " 10,\n",
       " 21,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 18,\n",
       " 14,\n",
       " 54,\n",
       " 50,\n",
       " 65,\n",
       " 27,\n",
       " 30,\n",
       " 29,\n",
       " 13,\n",
       " 19,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 16,\n",
       " 30,\n",
       " 24,\n",
       " 27,\n",
       " 24,\n",
       " 15,\n",
       " 17,\n",
       " 23,\n",
       " 21,\n",
       " 25,\n",
       " 13,\n",
       " 24,\n",
       " 17,\n",
       " 21,\n",
       " 16,\n",
       " 18,\n",
       " 21,\n",
       " 17,\n",
       " 23,\n",
       " 22,\n",
       " 20,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 20,\n",
       " 44,\n",
       " 15,\n",
       " 22,\n",
       " 15,\n",
       " 23,\n",
       " 14,\n",
       " 14,\n",
       " 19,\n",
       " 22,\n",
       " 15,\n",
       " 17,\n",
       " 13,\n",
       " 17,\n",
       " 10,\n",
       " 15,\n",
       " 23,\n",
       " 20,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 32,\n",
       " 17,\n",
       " 25,\n",
       " 27,\n",
       " 47,\n",
       " 17,\n",
       " 30,\n",
       " 17,\n",
       " 28,\n",
       " 22,\n",
       " 38,\n",
       " 32,\n",
       " 29,\n",
       " 35,\n",
       " 36,\n",
       " 29,\n",
       " 29,\n",
       " 24,\n",
       " 20,\n",
       " 20,\n",
       " 6,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 16,\n",
       " 23,\n",
       " 27,\n",
       " 13,\n",
       " 16,\n",
       " 25,\n",
       " 15,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 26,\n",
       " 19,\n",
       " 21,\n",
       " 17,\n",
       " 25,\n",
       " 23,\n",
       " 24,\n",
       " 29,\n",
       " 25,\n",
       " 26,\n",
       " 28,\n",
       " 30,\n",
       " 26,\n",
       " 27,\n",
       " 36,\n",
       " 21,\n",
       " 35,\n",
       " 34,\n",
       " 41,\n",
       " 22,\n",
       " 22,\n",
       " 13,\n",
       " 17,\n",
       " 18,\n",
       " 17,\n",
       " 15,\n",
       " 17,\n",
       " 15,\n",
       " 14,\n",
       " 10,\n",
       " 16,\n",
       " 11,\n",
       " 17,\n",
       " 11,\n",
       " 17,\n",
       " 13,\n",
       " 16,\n",
       " 18,\n",
       " 30,\n",
       " 14,\n",
       " 20,\n",
       " 30,\n",
       " 22,\n",
       " 17,\n",
       " 18,\n",
       " 9,\n",
       " 17,\n",
       " 21,\n",
       " 28,\n",
       " 13,\n",
       " 21,\n",
       " 20,\n",
       " 24,\n",
       " 17,\n",
       " 31,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 19,\n",
       " 18,\n",
       " 21,\n",
       " 19,\n",
       " 43,\n",
       " 44,\n",
       " 20,\n",
       " 47,\n",
       " 24,\n",
       " 30,\n",
       " 30,\n",
       " 20,\n",
       " 21,\n",
       " 16,\n",
       " 12,\n",
       " 26,\n",
       " 25,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 11,\n",
       " 19,\n",
       " 16,\n",
       " 15,\n",
       " 19,\n",
       " 22,\n",
       " 16,\n",
       " 46,\n",
       " 15,\n",
       " 19,\n",
       " 21,\n",
       " 16,\n",
       " 13,\n",
       " 17,\n",
       " 23,\n",
       " 23,\n",
       " 15,\n",
       " 37,\n",
       " 31,\n",
       " 10,\n",
       " 23,\n",
       " 17,\n",
       " 19,\n",
       " 13,\n",
       " 15,\n",
       " 16,\n",
       " 24,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 27,\n",
       " 21,\n",
       " 27,\n",
       " 20,\n",
       " 51,\n",
       " 14,\n",
       " 29,\n",
       " 13,\n",
       " 17,\n",
       " 22,\n",
       " 12,\n",
       " 20,\n",
       " 18,\n",
       " 12,\n",
       " 28,\n",
       " 41,\n",
       " 13,\n",
       " 15,\n",
       " 21,\n",
       " 16,\n",
       " 18,\n",
       " 23,\n",
       " 28,\n",
       " 17,\n",
       " 25,\n",
       " 13,\n",
       " 21,\n",
       " 17,\n",
       " 21,\n",
       " 19,\n",
       " 22,\n",
       " 26,\n",
       " 16,\n",
       " 23,\n",
       " 11,\n",
       " 25,\n",
       " 23,\n",
       " 31,\n",
       " 12,\n",
       " 28,\n",
       " 17,\n",
       " 16,\n",
       " 28,\n",
       " 16,\n",
       " 21,\n",
       " 22,\n",
       " 19,\n",
       " 33,\n",
       " 18,\n",
       " 23,\n",
       " 21,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 32,\n",
       " 27,\n",
       " 28,\n",
       " 17,\n",
       " 24,\n",
       " 16,\n",
       " 52,\n",
       " 60,\n",
       " 41,\n",
       " 14,\n",
       " 42,\n",
       " 13,\n",
       " 11,\n",
       " 28,\n",
       " 13,\n",
       " 35,\n",
       " 37,\n",
       " 19,\n",
       " 39,\n",
       " 40,\n",
       " 27,\n",
       " 23,\n",
       " 18,\n",
       " 19,\n",
       " 26,\n",
       " 31,\n",
       " 19,\n",
       " 18,\n",
       " 21,\n",
       " 30,\n",
       " 47,\n",
       " 13,\n",
       " 14,\n",
       " 23,\n",
       " 17,\n",
       " 17,\n",
       " 12,\n",
       " 9,\n",
       " 7,\n",
       " 13,\n",
       " 14,\n",
       " 18,\n",
       " 20,\n",
       " 23,\n",
       " 19,\n",
       " 43,\n",
       " 24,\n",
       " 17,\n",
       " 20,\n",
       " 25,\n",
       " 22,\n",
       " 25,\n",
       " 32,\n",
       " 33,\n",
       " 18,\n",
       " 19,\n",
       " 30,\n",
       " 21,\n",
       " 32,\n",
       " 19,\n",
       " 20,\n",
       " 29,\n",
       " 13,\n",
       " 18,\n",
       " 15,\n",
       " 13,\n",
       " 19,\n",
       " 17,\n",
       " 14,\n",
       " 11,\n",
       " 9,\n",
       " 26,\n",
       " ...]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (사전 계산) 각 문장들의 토큰 길이 구해놓기\n",
    "# corpus_size = len(corpus_mun)\n",
    "\n",
    "# max_len_list = []\n",
    "\n",
    "# for idx in range(corpus_size):\n",
    "#     encoded = tokenizer.encode(corpus_mun[idx])\n",
    "#     max_len_list.append(len(encoded))\n",
    "    \n",
    "# max_len_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42d91f",
   "metadata": {},
   "source": [
    "- 2. 반말체 학습(fine-tuning)용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c6aa184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27,\n",
       " 26,\n",
       " 19,\n",
       " 22,\n",
       " 11,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 14,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 14,\n",
       " 24,\n",
       " 20,\n",
       " 34,\n",
       " 22,\n",
       " 35,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 13,\n",
       " 11,\n",
       " 9,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 22,\n",
       " 24,\n",
       " 26,\n",
       " 27,\n",
       " 13,\n",
       " 51,\n",
       " 31,\n",
       " 48,\n",
       " 16,\n",
       " 10,\n",
       " 25,\n",
       " 11,\n",
       " 13,\n",
       " 8,\n",
       " 17,\n",
       " 14,\n",
       " 46,\n",
       " 21,\n",
       " 15,\n",
       " 22,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 34,\n",
       " 22,\n",
       " 18,\n",
       " 10,\n",
       " 18,\n",
       " 19,\n",
       " 24,\n",
       " 10,\n",
       " 17,\n",
       " 13,\n",
       " 15,\n",
       " 12,\n",
       " 13,\n",
       " 30,\n",
       " 15,\n",
       " 20,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 16,\n",
       " 21,\n",
       " 14,\n",
       " 27,\n",
       " 21,\n",
       " 8,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 16,\n",
       " 17,\n",
       " 14,\n",
       " 18,\n",
       " 16,\n",
       " 19,\n",
       " 77,\n",
       " 24,\n",
       " 31,\n",
       " 14,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 45,\n",
       " 27,\n",
       " 22,\n",
       " 15,\n",
       " 23,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 11,\n",
       " 16,\n",
       " 12,\n",
       " 35,\n",
       " 14,\n",
       " 11,\n",
       " 17,\n",
       " 11,\n",
       " 17,\n",
       " 15,\n",
       " 43,\n",
       " 40,\n",
       " 25,\n",
       " 31,\n",
       " 20,\n",
       " 15,\n",
       " 21,\n",
       " 18,\n",
       " 13,\n",
       " 13,\n",
       " 20,\n",
       " 11,\n",
       " 29,\n",
       " 11,\n",
       " 10,\n",
       " 11,\n",
       " 10,\n",
       " 13,\n",
       " 13,\n",
       " 24,\n",
       " 10,\n",
       " 19,\n",
       " 14,\n",
       " 9,\n",
       " 10,\n",
       " 17,\n",
       " 23,\n",
       " 23,\n",
       " 26,\n",
       " 22,\n",
       " 19,\n",
       " 18,\n",
       " 14,\n",
       " 17,\n",
       " 17,\n",
       " 21,\n",
       " 13,\n",
       " 17,\n",
       " 15,\n",
       " 13,\n",
       " 28,\n",
       " 9,\n",
       " 16,\n",
       " 24,\n",
       " 22,\n",
       " 25,\n",
       " 26,\n",
       " 13,\n",
       " 15,\n",
       " 14,\n",
       " 28,\n",
       " 25,\n",
       " 18,\n",
       " 18,\n",
       " 14,\n",
       " 14,\n",
       " 17,\n",
       " 34,\n",
       " 25,\n",
       " 20,\n",
       " 27,\n",
       " 17,\n",
       " 13,\n",
       " 7,\n",
       " 44,\n",
       " 10,\n",
       " 14,\n",
       " 34,\n",
       " 15,\n",
       " 12,\n",
       " 13,\n",
       " 18,\n",
       " 25,\n",
       " 18,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 21,\n",
       " 11,\n",
       " 16,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 19,\n",
       " 22,\n",
       " 28,\n",
       " 14,\n",
       " 17,\n",
       " 17,\n",
       " 46,\n",
       " 13,\n",
       " 23,\n",
       " 21,\n",
       " 14,\n",
       " 16,\n",
       " 18,\n",
       " 28,\n",
       " 11,\n",
       " 32,\n",
       " 20,\n",
       " 20,\n",
       " 14,\n",
       " 21,\n",
       " 19,\n",
       " 17,\n",
       " 12,\n",
       " 18,\n",
       " 19,\n",
       " 22,\n",
       " 21,\n",
       " 18,\n",
       " 29,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 17,\n",
       " 11,\n",
       " 18,\n",
       " 16,\n",
       " 20,\n",
       " 21,\n",
       " 14,\n",
       " 25,\n",
       " 21,\n",
       " 13,\n",
       " 15,\n",
       " 26,\n",
       " 33,\n",
       " 19,\n",
       " 25,\n",
       " 29,\n",
       " 19,\n",
       " 11,\n",
       " 12,\n",
       " 23,\n",
       " 34,\n",
       " 17,\n",
       " 30,\n",
       " 21,\n",
       " 10,\n",
       " 24,\n",
       " 10,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 12,\n",
       " 42,\n",
       " 27,\n",
       " 23,\n",
       " 31,\n",
       " 36,\n",
       " 25,\n",
       " 26,\n",
       " 56,\n",
       " 32,\n",
       " 14,\n",
       " 24,\n",
       " 34,\n",
       " 25,\n",
       " 24,\n",
       " 19,\n",
       " 25,\n",
       " 15,\n",
       " 28,\n",
       " 40,\n",
       " 23,\n",
       " 11,\n",
       " 17,\n",
       " 15,\n",
       " 20,\n",
       " 11,\n",
       " 19,\n",
       " 16,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 26,\n",
       " 17,\n",
       " 18,\n",
       " 17,\n",
       " 9,\n",
       " 26,\n",
       " 22,\n",
       " 20,\n",
       " 23,\n",
       " 14,\n",
       " 27,\n",
       " 13,\n",
       " 22,\n",
       " 21,\n",
       " 17,\n",
       " 10,\n",
       " 31,\n",
       " 33,\n",
       " 9,\n",
       " 26,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 11,\n",
       " 35,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 20,\n",
       " 15,\n",
       " 13,\n",
       " 18,\n",
       " 21,\n",
       " 17,\n",
       " 18,\n",
       " 24,\n",
       " 21,\n",
       " 23,\n",
       " 14,\n",
       " 7,\n",
       " 23,\n",
       " 21,\n",
       " 27,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 20,\n",
       " 39,\n",
       " 11,\n",
       " 16,\n",
       " 25,\n",
       " 24,\n",
       " 15,\n",
       " 19,\n",
       " 25,\n",
       " 20,\n",
       " 30,\n",
       " 18,\n",
       " 18,\n",
       " 28,\n",
       " 17,\n",
       " 17,\n",
       " 50,\n",
       " 38,\n",
       " 15,\n",
       " 21,\n",
       " 21,\n",
       " 13,\n",
       " 17,\n",
       " 16,\n",
       " 11,\n",
       " 8,\n",
       " 11,\n",
       " 12,\n",
       " 45,\n",
       " 35,\n",
       " 17,\n",
       " 14,\n",
       " 24,\n",
       " 25,\n",
       " 35,\n",
       " 23,\n",
       " 31,\n",
       " 14,\n",
       " 18,\n",
       " 27,\n",
       " 13,\n",
       " 16,\n",
       " 20,\n",
       " 14,\n",
       " 16,\n",
       " 10,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 19,\n",
       " 17,\n",
       " 14,\n",
       " 17,\n",
       " 12,\n",
       " 11,\n",
       " 17,\n",
       " 16,\n",
       " 29,\n",
       " 18,\n",
       " 26,\n",
       " 15,\n",
       " 27,\n",
       " 13,\n",
       " 13,\n",
       " 31,\n",
       " 10,\n",
       " 23,\n",
       " 16,\n",
       " 22,\n",
       " 40,\n",
       " 17,\n",
       " 21,\n",
       " 17,\n",
       " 13,\n",
       " 16,\n",
       " 11,\n",
       " 15,\n",
       " 15,\n",
       " 24,\n",
       " 27,\n",
       " 27,\n",
       " 9,\n",
       " 13,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 16,\n",
       " 27,\n",
       " 21,\n",
       " 14,\n",
       " 13,\n",
       " 14,\n",
       " 18,\n",
       " 21,\n",
       " 16,\n",
       " 21,\n",
       " 23,\n",
       " 10,\n",
       " 16,\n",
       " 14,\n",
       " 23,\n",
       " 29,\n",
       " 27,\n",
       " 30,\n",
       " 17,\n",
       " 38,\n",
       " 19,\n",
       " 17,\n",
       " 13,\n",
       " 19,\n",
       " 21,\n",
       " 15,\n",
       " 21,\n",
       " 10,\n",
       " 18,\n",
       " 14,\n",
       " 17,\n",
       " 23,\n",
       " 10,\n",
       " 31,\n",
       " 23,\n",
       " 17,\n",
       " 21,\n",
       " 22,\n",
       " 15,\n",
       " 15,\n",
       " 24,\n",
       " 21,\n",
       " 12,\n",
       " 19,\n",
       " 36,\n",
       " 25,\n",
       " 33,\n",
       " 15,\n",
       " 22,\n",
       " 38,\n",
       " 18,\n",
       " 14,\n",
       " 15,\n",
       " 24,\n",
       " 17,\n",
       " 28,\n",
       " 17,\n",
       " 14,\n",
       " 18,\n",
       " 20,\n",
       " 18,\n",
       " 18,\n",
       " 22,\n",
       " 19,\n",
       " 14,\n",
       " 25,\n",
       " 23,\n",
       " 19,\n",
       " 14,\n",
       " 20,\n",
       " 26,\n",
       " 23,\n",
       " 16,\n",
       " 14,\n",
       " 16,\n",
       " 12,\n",
       " 26,\n",
       " 16,\n",
       " 15,\n",
       " 24,\n",
       " 18,\n",
       " 14,\n",
       " 16,\n",
       " 8,\n",
       " 16,\n",
       " 14,\n",
       " 11,\n",
       " 18,\n",
       " 13,\n",
       " 18,\n",
       " 12,\n",
       " 27,\n",
       " 29,\n",
       " 23,\n",
       " 26,\n",
       " 14,\n",
       " 30,\n",
       " 33,\n",
       " 21,\n",
       " 21,\n",
       " 38,\n",
       " 19,\n",
       " 18,\n",
       " 29,\n",
       " 10,\n",
       " 12,\n",
       " 17,\n",
       " 13,\n",
       " 15,\n",
       " 23,\n",
       " 29,\n",
       " 30,\n",
       " 34,\n",
       " 32,\n",
       " 8,\n",
       " 11,\n",
       " 36,\n",
       " 27,\n",
       " 18,\n",
       " 27,\n",
       " 16,\n",
       " 18,\n",
       " 14,\n",
       " 20,\n",
       " 16,\n",
       " 15,\n",
       " 20,\n",
       " 20,\n",
       " 14,\n",
       " 7,\n",
       " 9,\n",
       " 23,\n",
       " 21,\n",
       " 20,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 10,\n",
       " 25,\n",
       " 13,\n",
       " 13,\n",
       " 28,\n",
       " 22,\n",
       " 20,\n",
       " 21,\n",
       " 24,\n",
       " 28,\n",
       " 12,\n",
       " 20,\n",
       " 18,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 27,\n",
       " 11,\n",
       " 25,\n",
       " 22,\n",
       " 24,\n",
       " 13,\n",
       " 19,\n",
       " 20,\n",
       " 57,\n",
       " 21,\n",
       " 26,\n",
       " 25,\n",
       " 16,\n",
       " 12,\n",
       " 22,\n",
       " 19,\n",
       " 9,\n",
       " 21,\n",
       " 19,\n",
       " 16,\n",
       " 15,\n",
       " 13,\n",
       " 47,\n",
       " 29,\n",
       " 12,\n",
       " 25,\n",
       " 21,\n",
       " 13,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 14,\n",
       " 18,\n",
       " 10,\n",
       " 15,\n",
       " 9,\n",
       " 11,\n",
       " 18,\n",
       " 13,\n",
       " 13,\n",
       " 15,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 18,\n",
       " 12,\n",
       " 19,\n",
       " 26,\n",
       " 26,\n",
       " 18,\n",
       " 12,\n",
       " 13,\n",
       " 9,\n",
       " 15,\n",
       " 16,\n",
       " 25,\n",
       " 22,\n",
       " 20,\n",
       " 18,\n",
       " 26,\n",
       " 22,\n",
       " 19,\n",
       " 25,\n",
       " 22,\n",
       " 23,\n",
       " 20,\n",
       " 16,\n",
       " 11,\n",
       " 41,\n",
       " 21,\n",
       " 16,\n",
       " 36,\n",
       " 32,\n",
       " 36,\n",
       " 28,\n",
       " 36,\n",
       " 34,\n",
       " 27,\n",
       " 22,\n",
       " 20,\n",
       " 24,\n",
       " 29,\n",
       " 16,\n",
       " 9,\n",
       " 40,\n",
       " 9,\n",
       " 18,\n",
       " 29,\n",
       " 14,\n",
       " 12,\n",
       " 14,\n",
       " 24,\n",
       " 19,\n",
       " 17,\n",
       " 17,\n",
       " 19,\n",
       " 16,\n",
       " 17,\n",
       " 19,\n",
       " 15,\n",
       " 28,\n",
       " 22,\n",
       " 19,\n",
       " 15,\n",
       " 14,\n",
       " 12,\n",
       " 32,\n",
       " 29,\n",
       " 15,\n",
       " 24,\n",
       " 13,\n",
       " 13,\n",
       " 17,\n",
       " 17,\n",
       " 8,\n",
       " 23,\n",
       " 17,\n",
       " 25,\n",
       " 13,\n",
       " 15,\n",
       " 23,\n",
       " 27,\n",
       " 14,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 26,\n",
       " 19,\n",
       " 10,\n",
       " 11,\n",
       " 21,\n",
       " 22,\n",
       " 20,\n",
       " 23,\n",
       " 14,\n",
       " 14,\n",
       " 18,\n",
       " 22,\n",
       " 19,\n",
       " 14,\n",
       " 16,\n",
       " 20,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 14,\n",
       " 14,\n",
       " 24,\n",
       " 15,\n",
       " 19,\n",
       " 27,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 22,\n",
       " 13,\n",
       " 18,\n",
       " 33,\n",
       " 18,\n",
       " 24,\n",
       " 23,\n",
       " 12,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 14,\n",
       " 13,\n",
       " 14,\n",
       " 19,\n",
       " 15,\n",
       " 24,\n",
       " 13,\n",
       " 19,\n",
       " 12,\n",
       " 36,\n",
       " 15,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 14,\n",
       " 18,\n",
       " 19,\n",
       " 16,\n",
       " 17,\n",
       " 21,\n",
       " 27,\n",
       " 16,\n",
       " 12,\n",
       " 15,\n",
       " 15,\n",
       " 23,\n",
       " 38,\n",
       " 23,\n",
       " 20,\n",
       " 21,\n",
       " 15,\n",
       " 12,\n",
       " 16,\n",
       " 10,\n",
       " 18,\n",
       " 19,\n",
       " 10,\n",
       " 20,\n",
       " 28,\n",
       " 13,\n",
       " 15,\n",
       " 20,\n",
       " 19,\n",
       " 16,\n",
       " 14,\n",
       " 12,\n",
       " 15,\n",
       " 16,\n",
       " 26,\n",
       " 27,\n",
       " 15,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 16,\n",
       " 20,\n",
       " 20,\n",
       " 15,\n",
       " 16,\n",
       " 10,\n",
       " 42,\n",
       " 20,\n",
       " 18,\n",
       " 27,\n",
       " 19,\n",
       " 12,\n",
       " 28,\n",
       " 18,\n",
       " 23,\n",
       " 26,\n",
       " 20,\n",
       " 20,\n",
       " 15,\n",
       " 16,\n",
       " 14,\n",
       " 17,\n",
       " 22,\n",
       " 29,\n",
       " 28,\n",
       " 30,\n",
       " 19,\n",
       " 14,\n",
       " 10,\n",
       " 12,\n",
       " 17,\n",
       " 13,\n",
       " 24,\n",
       " 11,\n",
       " 15,\n",
       " 14,\n",
       " 40,\n",
       " 19,\n",
       " 45,\n",
       " 24,\n",
       " 19,\n",
       " 14,\n",
       " 11,\n",
       " 13,\n",
       " 24,\n",
       " 8,\n",
       " 18,\n",
       " 11,\n",
       " 15,\n",
       " 25,\n",
       " 10,\n",
       " 14,\n",
       " 25,\n",
       " 36,\n",
       " 17,\n",
       " 16,\n",
       " 19,\n",
       " 32,\n",
       " 40,\n",
       " 34,\n",
       " 14,\n",
       " 33,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 29,\n",
       " 25,\n",
       " 13,\n",
       " 21,\n",
       " 14,\n",
       " 17,\n",
       " 23,\n",
       " 10,\n",
       " 14,\n",
       " 25,\n",
       " 16,\n",
       " 22,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 28,\n",
       " 17,\n",
       " 13,\n",
       " 37,\n",
       " 26,\n",
       " 26,\n",
       " 38,\n",
       " 33,\n",
       " 23,\n",
       " 25,\n",
       " 9,\n",
       " 17,\n",
       " 16,\n",
       " 25,\n",
       " 34,\n",
       " 28,\n",
       " 18,\n",
       " 14,\n",
       " 25,\n",
       " 24,\n",
       " 29,\n",
       " 9,\n",
       " 23,\n",
       " 16,\n",
       " 19,\n",
       " 24,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 11,\n",
       " 17,\n",
       " 16,\n",
       " 23,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 16,\n",
       " 15,\n",
       " 17,\n",
       " 14,\n",
       " 16,\n",
       " 25,\n",
       " 16,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 10,\n",
       " 27,\n",
       " 23,\n",
       " 32,\n",
       " 16,\n",
       " 20,\n",
       " 18,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 27,\n",
       " 29,\n",
       " 18,\n",
       " 16,\n",
       " 31,\n",
       " 15,\n",
       " 23,\n",
       " 22,\n",
       " 28,\n",
       " 11,\n",
       " 12,\n",
       " 27,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 19,\n",
       " 14,\n",
       " 24,\n",
       " 28,\n",
       " 9,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 20,\n",
       " 15,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 12,\n",
       " 15,\n",
       " 12,\n",
       " 14,\n",
       " 23,\n",
       " 15,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 14,\n",
       " 17,\n",
       " 40,\n",
       " 30,\n",
       " 17,\n",
       " 10,\n",
       " 15,\n",
       " 14,\n",
       " 20,\n",
       " 21,\n",
       " 33,\n",
       " 25,\n",
       " 24,\n",
       " 26,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_size = len(corpus_ban)\n",
    "\n",
    "max_len_list = []\n",
    "\n",
    "for idx in range(corpus_size):\n",
    "    encoded = tokenizer.encode(corpus_ban[idx])\n",
    "    max_len_list.append(len(encoded))\n",
    "    \n",
    "max_len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47999ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection = []\n",
    "\n",
    "# for i in range(mask_arr.shape[0]):\n",
    "#     selection.append(\n",
    "#         torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f3c1772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5a4bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr[0][19] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b6bc5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr[0] # 변화 적용됨을 확인함 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21eabdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr[0][19-2 : 19+4] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4938ac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True,  True,  True,\n",
       "         True,  True,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1896bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196327, 128])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61d0ef78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 3671, 2145,  ...,    0,    0,    0],\n",
       "        [   2,   11, 3854,  ...,    0,    0,    0],\n",
       "        [   2, 3686, 6431,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2, 5217, 2440,  ...,    0,    0,    0],\n",
       "        [   2, 7275, 6551,  ...,    0,    0,    0],\n",
       "        [   2, 5217, 2440,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[   2, 3671, 2145,  ...,    0,    0,    0],\n",
       "        [   2,   11, 3854,  ...,    0,    0,    0],\n",
       "        [   2, 3686, 6431,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2, 5217, 2440,  ...,    0,    0,    0],\n",
       "        [   2, 7275, 6551,  ...,    0,    0,    0],\n",
       "        [   2, 5217, 2440,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b72db120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  3671,  2145,  7249, 25859, 27135,    11,  3854,  4857,  2208,\n",
      "         2731,    11,  1498,  1428,  2259])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "tensor([ 601, 2073,   35,    3,    0,    0,    0,    0])\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids[0, 0:19-4])\n",
    "print(inputs.attention_mask[0, 0:19-4])\n",
    "print()\n",
    "print(inputs.input_ids[0, 19-4:19+4])\n",
    "print(inputs.attention_mask[0, 19-4:19+4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8657914",
   "metadata": {},
   "source": [
    "- 'attention_mask'에 변화주는 파트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d761e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 중요 !! ##########\n",
    "for i in range(corpus_size):\n",
    "    leng = max_len_list[i]\n",
    "    inputs.attention_mask[i, 0:leng-4] = 0 \n",
    "    inputs.attention_mask[i, leng-4:leng+4] = 1 # 1 로 바꿈 (masking해서 가르칠 대상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a82954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  3671,  2145,  7249, 25859, 27135,    11,  3854,  4857,  2208,\n",
      "         2731,    11,  1498,  1428,  2259])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "tensor([ 601, 2073,   35,    3,    0,    0,    0,    0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids[0, 0:19-4])\n",
    "print(inputs.attention_mask[0, 0:19-4])\n",
    "print()\n",
    "print(inputs.input_ids[0, 19-4:19+4])\n",
    "print(inputs.attention_mask[0, 19-4:19+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "597c44d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba742d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2e06e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d8102f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acef982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1a58e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##device = torch.device('cuda:0')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04de26cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = ('cuda:0')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f35e85cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33f351ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyunkyung_lee/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 0/6639 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "Epoch 0: 100%|██████████| 6639/6639 [35:22<00:00,  3.13it/s, loss=1.96e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(1.9619e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "* * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6639/6639 [35:31<00:00,  3.11it/s, loss=7.67e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(7.6669e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "* * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 6639/6639 [35:17<00:00,  3.13it/s, loss=4.64e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor(4.6444e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "* * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 6639/6639 [35:37<00:00,  3.11it/s, loss=1.66e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor(1.6633e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "* * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 6639/6639 [35:17<00:00,  3.13it/s, loss=1.95e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor(1.9465e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "* * * * *\n"
     ]
    }
   ],
   "source": [
    "optim = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 5 ####\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    \n",
    "    print(epoch)\n",
    "    print(loss)\n",
    "    print('* * * * *')\n",
    "    ### 1. 문어체 학습 시\n",
    "    #PATH = 'model/padded_MLM_train_mun_{0}_epoch'.format(epoch)\n",
    "    \n",
    "    ### 2. 반말체 학습 시\n",
    "    PATH = 'model/padded_MLM_train_ban_{0}_epoch'.format(epoch)\n",
    "    torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6584c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
